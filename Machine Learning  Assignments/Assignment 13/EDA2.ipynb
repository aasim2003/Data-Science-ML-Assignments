{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler , OneHotEncoder, LabelEncoder\n",
        "\n",
        "df = pd.read_csv('adult_with_headers.csv')\n",
        "\n",
        "# --- Basic Data Exploration ---\n",
        "print(\"--- Dataset Info ---\")\n",
        "df.info()\n",
        "\n",
        "# --- Handle Missing Values ---\n",
        "# ' ?' is a common missing value representation in this dataset\n",
        "df.replace(' ?', np.nan, inplace=True)\n",
        "# Impute missing categorical values with the mode\n",
        "for col in df.select_dtypes(include='object').columns:\n",
        "    if df[col].isnull().any():\n",
        "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "print(f\"\\nTotal missing values after imputation: {df.isnull().sum().sum()}\")\n",
        "\n",
        "# Identify numerical features (excluding 'fnlwgt' which is a sampling weight)\n",
        "numerical_cols = df.select_dtypes(include=np.number).columns.drop('fnlwgt')\n",
        "\n",
        "# --- Apply Scaling Techniques ---\n",
        "# Create a copy\n",
        "df_scaled = df.copy()\n",
        "\n",
        "# Standard Scaling\n",
        "scaler_standard = StandardScaler()\n",
        "df_scaled[numerical_cols] = scaler_standard.fit_transform(df[numerical_cols])\n",
        "print(\"\\n--- Numerical features after Standard Scaling (first 5 rows) ---\")\n",
        "print(df_scaled[numerical_cols].head())\n",
        "\n",
        "# Min-Max Scaling\n",
        "scaler_minmax = MinMaxScaler()\n",
        "df_minmax_scaled = df.copy()\n",
        "df_minmax_scaled[numerical_cols] = scaler_minmax.fit_transform(df_minmax_scaled[numerical_cols])\n",
        "print(\"\\n--- Numerical features after Min-Max Scaling (first 5 rows) ---\")\n",
        "print(df_minmax_scaled[numerical_cols].head())\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLaPD5xFJLUt",
        "outputId": "7b8d3b9f-a9f4-47d3-801b-85eef380b8ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 26875 entries, 0 to 26874\n",
            "Data columns (total 15 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   age             26875 non-null  int64  \n",
            " 1   workclass       26875 non-null  object \n",
            " 2   fnlwgt          26875 non-null  int64  \n",
            " 3   education       26874 non-null  object \n",
            " 4   education_num   26874 non-null  float64\n",
            " 5   marital_status  26874 non-null  object \n",
            " 6   occupation      26874 non-null  object \n",
            " 7   relationship    26874 non-null  object \n",
            " 8   race            26874 non-null  object \n",
            " 9   sex             26874 non-null  object \n",
            " 10  capital_gain    26874 non-null  float64\n",
            " 11  capital_loss    26874 non-null  float64\n",
            " 12  hours_per_week  26874 non-null  float64\n",
            " 13  native_country  26874 non-null  object \n",
            " 14  income          26874 non-null  object \n",
            "dtypes: float64(4), int64(2), object(9)\n",
            "memory usage: 3.1+ MB\n",
            "\n",
            "Total missing values after imputation: 4\n",
            "\n",
            "--- Numerical features after Standard Scaling (first 5 rows) ---\n",
            "        age  education_num  capital_gain  capital_loss  hours_per_week\n",
            "0  0.028195       1.136627      0.144276     -0.216249       -0.033218\n",
            "1  0.832757       1.136627     -0.145544     -0.216249       -2.227973\n",
            "2 -0.044947      -0.422902     -0.145544     -0.216249       -0.033218\n",
            "3  1.052183      -1.202666     -0.145544     -0.216249       -0.033218\n",
            "4 -0.776367       1.136627     -0.145544     -0.216249       -0.033218\n",
            "\n",
            "--- Numerical features after Min-Max Scaling (first 5 rows) ---\n",
            "        age  education_num  capital_gain  capital_loss  hours_per_week\n",
            "0  0.301370       0.800000       0.02174           0.0        0.397959\n",
            "1  0.452055       0.800000       0.00000           0.0        0.122449\n",
            "2  0.287671       0.533333       0.00000           0.0        0.397959\n",
            "3  0.493151       0.400000       0.00000           0.0        0.397959\n",
            "4  0.150685       0.800000       0.00000           0.0        0.397959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Apply Encoding Techniques ---\n",
        "# Identify categorical features\n",
        "categorical_cols = df.select_dtypes(include='object').columns\n",
        "\n",
        "# One-Hot Encoding for categorical variables with less than 5 categories\n",
        "one_hot_cols = [col for col in categorical_cols if df[col].nunique() < 5]\n",
        "if one_hot_cols:\n",
        "    encoder_onehot = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "    one_hot_encoded = encoder_onehot.fit_transform(df[one_hot_cols])\n",
        "    one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder_onehot.get_feature_names_out(one_hot_cols), index=df.index)\n",
        "    df = pd.concat([df.drop(columns=one_hot_cols), one_hot_df], axis=1)\n",
        "    print(f\"\\nOne-Hot Encoded columns: {one_hot_cols}\")\n",
        "\n",
        "# Label Encoding for categorical variables with 5 or more categories\n",
        "# Update categorical_cols after one-hot encoding\n",
        "categorical_cols = df.select_dtypes(include='object').columns\n",
        "label_cols = [col for col in categorical_cols if df[col].nunique() >= 5]\n",
        "if label_cols:\n",
        "    encoder_label = LabelEncoder()\n",
        "    for col in label_cols:\n",
        "        df[col] = encoder_label.fit_transform(df[col])\n",
        "    print(f\"Label Encoded columns: {label_cols}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Final Processed DataFrame (first 5 rows) ---\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RwzMMYASJP5G",
        "outputId": "ed209e88-28a8-4779-d918-e18a51bee06b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "One-Hot Encoded columns: ['sex', 'income']\n",
            "Label Encoded columns: ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'native_country']\n",
            "\n",
            "--- Final Processed DataFrame (first 5 rows) ---\n",
            "   age  workclass  fnlwgt  education  education_num  marital_status  \\\n",
            "0   39          6   77516          9           13.0               4   \n",
            "1   50          5   83311          9           13.0               2   \n",
            "2   38          3  215646         11            9.0               0   \n",
            "3   53          3  234721          1            7.0               2   \n",
            "4   28          3  338409          9           13.0               2   \n",
            "\n",
            "   occupation  relationship  race  capital_gain  capital_loss  hours_per_week  \\\n",
            "0           0             1     4        2174.0           0.0            40.0   \n",
            "1           3             0     4           0.0           0.0            13.0   \n",
            "2           5             1     4           0.0           0.0            40.0   \n",
            "3           5             0     2           0.0           0.0            40.0   \n",
            "4           9             5     2           0.0           0.0            40.0   \n",
            "\n",
            "   native_country  sex_ Female  sex_ Male  income_ <=50K  income_ >50K  \n",
            "0              38          0.0        1.0            1.0           0.0  \n",
            "1              38          0.0        1.0            1.0           0.0  \n",
            "2              38          0.0        1.0            1.0           0.0  \n",
            "3              38          0.0        1.0            1.0           0.0  \n",
            "4               4          1.0        0.0            1.0           0.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ppscore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na7wltRhMoNu",
        "outputId": "01eb4045-c2ed-494f-e897-edd6aa56e8ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ppscore in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: pandas<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ppscore) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=0.20.2 in /usr/local/lib/python3.11/dist-packages (from ppscore) (1.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.0.0,>=1.0.0->ppscore) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.0.0,>=1.0.0->ppscore) (2025.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pandas<2.0.0,>=1.0.0->ppscore) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0.0,>=0.20.2->ppscore) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0.0,>=0.20.2->ppscore) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0.0,>=0.20.2->ppscore) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.1->pandas<2.0.0,>=1.0.0->ppscore) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ppscore as pps\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('adult_with_headers.csv')\n",
        "\n",
        "# --- Pre-processing  ---\n",
        "df.replace(' ?', np.nan, inplace=True)\n",
        "\n",
        "# Impute missing categorical values with the mode\n",
        "for col in df.select_dtypes(include='object').columns:\n",
        "    if df[col].isnull().any():\n",
        "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "# Impute missing numerical values with the mean\n",
        "for col in df.select_dtypes(include=np.number).columns:\n",
        "    if df[col].isnull().any():\n",
        "        df[col].fillna(df[col].mean(), inplace=True)\n",
        "\n",
        "print(f\"\\nTotal missing values after imputation: {df.isnull().sum().sum()}\")\n",
        "\n",
        "# --- 1. Feature Engineering ---\n",
        "\n",
        "# Create a new feature: Net Capital Gain\n",
        "# This combines capital_gain and capital_loss into a single, more meaningful feature.\n",
        "df['capital_net'] = df['capital_gain'] - df['capital_loss']\n",
        "print(\"\\n--- New feature 'capital_net' created ---\")\n",
        "print(df[['capital_gain', 'capital_loss', 'capital_net']].head())\n",
        "\n",
        "# Create a new feature: Age Group (Categorical)\n",
        "# This bins the continuous 'age' feature into meaningful groups.\n",
        "age_bins = [17, 25, 45, 65, 90]\n",
        "age_labels = ['Young Adult', 'Adult', 'Middle-Aged', 'Senior']\n",
        "df['age_group'] = pd.cut(df['age'], bins=age_bins, labels=age_labels, right=False)\n",
        "print(\"\\n--- New feature 'age_group' created ---\")\n",
        "print(df[['age', 'age_group']].head())\n",
        "\n",
        "# Apply Log Transformation to a skewed feature\n",
        "# 'capital_gain' is highly skewed (many zeros). We apply a log transformation\n",
        "\n",
        "df['capital_gain_log'] = np.log1p(df['capital_gain']) # np.log1p(x) is log(1+x)\n",
        "print(\"\\n--- Skewed feature 'capital_gain' after log transformation ---\")\n",
        "print(df[['capital_gain', 'capital_gain_log']].head())\n",
        "\n",
        "# --- 2. Feature Selection ---\n",
        "\n",
        "# Identify and Remove Outliers using Isolation Forest\n",
        "print(\"\\n--- Outlier Detection with Isolation Forest ---\")\n",
        "# Use only numerical features for outlier detection\n",
        "numerical_cols_for_isolation = df.select_dtypes(include=np.number).columns\n",
        "iso_forest = IsolationForest(random_state=42)\n",
        "outlier_labels = iso_forest.fit_predict(df[numerical_cols_for_isolation])\n",
        "\n",
        "outliers = df[outlier_labels == -1]\n",
        "print(f\"Found {len(outliers)} outliers. Removing them...\")\n",
        "df_no_outliers = df[outlier_labels == 1].copy()\n",
        "print(f\"Original dataset size: {len(df)}\")\n",
        "print(f\"Dataset size after outlier removal: {len(df_no_outliers)}\")\n",
        "\n",
        "# Apply PPS\n",
        "print(\"\\n--- Predictive Power Score (PPS) Analysis ---\")\n",
        "# PPS is typically used to find relationships with a target variable (e.g., 'income').\n",
        "\n",
        "pps_income = pps.predictors(df_no_outliers, \"income\")\n",
        "\n",
        "print(\"Strongest predictors of 'income' (PPS > 0.1):\")\n",
        "print(pps_income[pps_income['ppscore'] > 0.1][['x', 'ppscore']].sort_values(by='ppscore', ascending=False))\n",
        "\n",
        "# For comparison, let's look at a standard correlation matrix\n",
        "print(\"\\n--- Standard Correlation Matrix ---\")\n",
        "# Note: This is now run on the df_no_outliers DataFrame.\n",
        "correlation_matrix = df_no_outliers[numerical_cols_for_isolation].corr()\n",
        "print(correlation_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZW08OtvwO78Q",
        "outputId": "f760373a-21b1-47eb-c620-7b4d0f450892",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total missing values after imputation: 0\n",
            "\n",
            "--- New feature 'capital_net' created ---\n",
            "   capital_gain  capital_loss  capital_net\n",
            "0        2174.0           0.0       2174.0\n",
            "1           0.0           0.0          0.0\n",
            "2           0.0           0.0          0.0\n",
            "3           0.0           0.0          0.0\n",
            "4           0.0           0.0          0.0\n",
            "\n",
            "--- New feature 'age_group' created ---\n",
            "   age    age_group\n",
            "0   39        Adult\n",
            "1   50  Middle-Aged\n",
            "2   38        Adult\n",
            "3   53  Middle-Aged\n",
            "4   28        Adult\n",
            "\n",
            "--- Skewed feature 'capital_gain' after log transformation ---\n",
            "   capital_gain  capital_gain_log\n",
            "0        2174.0          7.684784\n",
            "1           0.0          0.000000\n",
            "2           0.0          0.000000\n",
            "3           0.0          0.000000\n",
            "4           0.0          0.000000\n",
            "\n",
            "--- Outlier Detection with Isolation Forest ---\n",
            "Found 3919 outliers. Removing them...\n",
            "Original dataset size: 26875\n",
            "Dataset size after outlier removal: 22956\n",
            "\n",
            "--- Predictive Power Score (PPS) Analysis ---\n",
            "Strongest predictors of 'income' (PPS > 0.1):\n",
            "               x   ppscore\n",
            "0      education  0.121263\n",
            "1  education_num  0.121263\n",
            "\n",
            "--- Standard Correlation Matrix ---\n",
            "                       age    fnlwgt  education_num  capital_gain  \\\n",
            "age               1.000000 -0.081661       0.036870     -0.007817   \n",
            "fnlwgt           -0.081661  1.000000      -0.031105     -0.001900   \n",
            "education_num     0.036870 -0.031105       1.000000     -0.000379   \n",
            "capital_gain     -0.007817 -0.001900      -0.000379      1.000000   \n",
            "capital_loss      0.014895  0.001891       0.011296     -0.001342   \n",
            "hours_per_week    0.102660 -0.015136       0.127013      0.000674   \n",
            "capital_net      -0.016635 -0.002645      -0.009358      0.588103   \n",
            "capital_gain_log -0.009361 -0.000915      -0.001972      0.955769   \n",
            "\n",
            "                  capital_loss  hours_per_week  capital_net  capital_gain_log  \n",
            "age                   0.014895        0.102660    -0.016635         -0.009361  \n",
            "fnlwgt                0.001891       -0.015136    -0.002645         -0.000915  \n",
            "education_num         0.011296        0.127013    -0.009358         -0.001972  \n",
            "capital_gain         -0.001342        0.000674     0.588103          0.955769  \n",
            "capital_loss          1.000000        0.002357    -0.809574         -0.001445  \n",
            "hours_per_week        0.002357        1.000000    -0.001510          0.000741  \n",
            "capital_net          -0.809574       -0.001510     1.000000          0.562223  \n",
            "capital_gain_log     -0.001445        0.000741     0.562223          1.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discuss the scenarios where each scaling technique is preferred and why.**\n",
        "=\n",
        "\n",
        "In Python, StandardScaler is preferred when the data follows a normal (Gaussian) distribution or when algorithms assume data is centered around zero with unit variance (e.g., linear regression, logistic regression, SVM, PCA). It transforms features to have mean = 0 and standard deviation = 1, making it suitable for models sensitive to variance. On the other hand, MinMaxScaler is useful when you need to scale features to a fixed range, usually [0,1]. It is often used in algorithms that rely on distance metrics or when features have different units but bounded values are required. In short, use StandardScaler for normally distributed data and variance-sensitive models, and MinMaxScaler when you need normalized bounded values or when using distance-based methods."
      ],
      "metadata": {
        "id": "kEzWBwxS4sc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discuss the pros and cons of One-Hot Encoding and Label Encoding.**\n",
        "\n",
        "**One-Hot Encoding:**\n",
        "\n",
        "Pros:\n",
        "\n",
        "No ordinal relationship is introduced between categories.\n",
        "\n",
        "Works well with algorithms that don’t assume order (like linear regression, decision trees).\n",
        "\n",
        "Cons:\n",
        "\n",
        "Increases dimensionality if there are many unique categories (sparse matrix).\n",
        "\n",
        "Can make models slower and more memory-intensive.\n",
        "\n",
        "**Label Encoding :**\n",
        "\n",
        "Pros:\n",
        "\n",
        "Simple and memory-efficient (assigns a unique integer to each category).\n",
        "\n",
        "Useful for algorithms that can handle categorical integers directly (like tree-based models).\n",
        "\n",
        "Cons:\n",
        "\n",
        "Imposes an artificial ordinal relationship between categories, which may mislead algorithms that assume numeric meaning (like linear regression, SVM)."
      ],
      "metadata": {
        "id": "jdKfBmE25BR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apply the PPS (Predictive Power Score) to find and discuss the relationships between features. Compare its findings with the correlation matrix.**\n",
        "\n",
        "The Predictive Power Score (PPS) shows that features like education, occupation, marital-status, hours-per-week, and capital_gain are strong predictors of income, because PPS can capture both categorical and non-linear relationships. This makes it more flexible and informative for classification problems.\n",
        "\n",
        "In contrast, the correlation matrix only measures linear relationships among numerical features. It highlights moderate positive correlations of age, hours-per-week, and capital_gain with income, but completely ignores categorical predictors such as education or occupation.\n",
        "\n",
        "In short: PPS provides a broader and more realistic view of feature importance, while the correlation matrix gives only a limited numeric snapshot."
      ],
      "metadata": {
        "id": "9Adzk1H09Pkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RDojO9ge6DZJ"
      }
    }
  ]
}