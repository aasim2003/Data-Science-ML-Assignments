{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUK6LW8_aOPt",
        "outputId": "c62cc60f-aba6-4672-8fc4-9a1b4758354c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully!\n",
            "  letter  xbox  ybox  width  height  onpix  xbar  ybar  x2bar  y2bar  xybar  \\\n",
            "0      T     2     8      3       5      1     8    13      0      6      6   \n",
            "1      I     5    12      3       7      2    10     5      5      4     13   \n",
            "2      D     4    11      6       8      6    10     6      2      6     10   \n",
            "3      N     7    11      6       6      3     5     9      4      6      4   \n",
            "4      G     2     1      3       1      1     8     6      6      6      6   \n",
            "\n",
            "   x2ybar  xy2bar  xedge  xedgey  yedge  yedgex  \n",
            "0      10       8      0       8      0       8  \n",
            "1       3       9      2       8      4      10  \n",
            "2       3       7      3       7      3       9  \n",
            "3       4      10      6      10      2       8  \n",
            "4       5       9      1       7      5      10  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the dataset directly by its name\n",
        "data = pd.read_csv('Alphabets_data.csv')\n",
        "\n",
        "# Show the first few rows to make sure it's loaded\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Data Exploration ---\n",
        "print(\"Dataset Information:\")\n",
        "data.info()\n",
        "\n",
        "print(\"\\nStatistical Summary:\")\n",
        "print(data.describe())\n",
        "\n",
        "# Summarize key features\n",
        "num_samples = data.shape[0]\n",
        "num_features = data.shape[1] - 1  # Subtracting the target column 'letter'\n",
        "num_classes = data['letter'].nunique()\n",
        "\n",
        "print(f\"\\nNumber of Samples: {num_samples}\")\n",
        "print(f\"Number of Features: {num_features}\")\n",
        "print(f\"Number of Classes: {num_classes}\")\n",
        "\n",
        "# --- 2. Data Preprocessing ---\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values Check:\")\n",
        "print(data.isnull().sum())\n",
        "# No missing values found, so no imputation is needed.\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop('letter', axis=1)\n",
        "y = data['letter']\n",
        "\n",
        "# Normalize the feature data to a range of 0-1\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Encode the categorical target labels into integers\n",
        "encoder = LabelEncoder()\n",
        "y_encoded = encoder.fit_transform(y)\n",
        "\n",
        "# Convert integer labels to one-hot encoded vectors\n",
        "y_one_hot = to_categorical(y_encoded)\n",
        "\n",
        "print(\"\\nShape of scaled features (X):\", X_scaled.shape)\n",
        "print(\"Shape of one-hot encoded target (y):\", y_one_hot.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2U2pWRzygtaY",
        "outputId": "4c093c71-2789-4c09-88e6-cec17f662301"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20000 entries, 0 to 19999\n",
            "Data columns (total 17 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   letter  20000 non-null  object\n",
            " 1   xbox    20000 non-null  int64 \n",
            " 2   ybox    20000 non-null  int64 \n",
            " 3   width   20000 non-null  int64 \n",
            " 4   height  20000 non-null  int64 \n",
            " 5   onpix   20000 non-null  int64 \n",
            " 6   xbar    20000 non-null  int64 \n",
            " 7   ybar    20000 non-null  int64 \n",
            " 8   x2bar   20000 non-null  int64 \n",
            " 9   y2bar   20000 non-null  int64 \n",
            " 10  xybar   20000 non-null  int64 \n",
            " 11  x2ybar  20000 non-null  int64 \n",
            " 12  xy2bar  20000 non-null  int64 \n",
            " 13  xedge   20000 non-null  int64 \n",
            " 14  xedgey  20000 non-null  int64 \n",
            " 15  yedge   20000 non-null  int64 \n",
            " 16  yedgex  20000 non-null  int64 \n",
            "dtypes: int64(16), object(1)\n",
            "memory usage: 2.6+ MB\n",
            "\n",
            "Statistical Summary:\n",
            "               xbox          ybox         width       height         onpix  \\\n",
            "count  20000.000000  20000.000000  20000.000000  20000.00000  20000.000000   \n",
            "mean       4.023550      7.035500      5.121850      5.37245      3.505850   \n",
            "std        1.913212      3.304555      2.014573      2.26139      2.190458   \n",
            "min        0.000000      0.000000      0.000000      0.00000      0.000000   \n",
            "25%        3.000000      5.000000      4.000000      4.00000      2.000000   \n",
            "50%        4.000000      7.000000      5.000000      6.00000      3.000000   \n",
            "75%        5.000000      9.000000      6.000000      7.00000      5.000000   \n",
            "max       15.000000     15.000000     15.000000     15.00000     15.000000   \n",
            "\n",
            "               xbar          ybar         x2bar         y2bar         xybar  \\\n",
            "count  20000.000000  20000.000000  20000.000000  20000.000000  20000.000000   \n",
            "mean       6.897600      7.500450      4.628600      5.178650      8.282050   \n",
            "std        2.026035      2.325354      2.699968      2.380823      2.488475   \n",
            "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
            "25%        6.000000      6.000000      3.000000      4.000000      7.000000   \n",
            "50%        7.000000      7.000000      4.000000      5.000000      8.000000   \n",
            "75%        8.000000      9.000000      6.000000      7.000000     10.000000   \n",
            "max       15.000000     15.000000     15.000000     15.000000     15.000000   \n",
            "\n",
            "            x2ybar        xy2bar         xedge        xedgey         yedge  \\\n",
            "count  20000.00000  20000.000000  20000.000000  20000.000000  20000.000000   \n",
            "mean       6.45400      7.929000      3.046100      8.338850      3.691750   \n",
            "std        2.63107      2.080619      2.332541      1.546722      2.567073   \n",
            "min        0.00000      0.000000      0.000000      0.000000      0.000000   \n",
            "25%        5.00000      7.000000      1.000000      8.000000      2.000000   \n",
            "50%        6.00000      8.000000      3.000000      8.000000      3.000000   \n",
            "75%        8.00000      9.000000      4.000000      9.000000      5.000000   \n",
            "max       15.00000     15.000000     15.000000     15.000000     15.000000   \n",
            "\n",
            "            yedgex  \n",
            "count  20000.00000  \n",
            "mean       7.80120  \n",
            "std        1.61747  \n",
            "min        0.00000  \n",
            "25%        7.00000  \n",
            "50%        8.00000  \n",
            "75%        9.00000  \n",
            "max       15.00000  \n",
            "\n",
            "Number of Samples: 20000\n",
            "Number of Features: 16\n",
            "Number of Classes: 26\n",
            "\n",
            "Missing Values Check:\n",
            "letter    0\n",
            "xbox      0\n",
            "ybox      0\n",
            "width     0\n",
            "height    0\n",
            "onpix     0\n",
            "xbar      0\n",
            "ybar      0\n",
            "x2bar     0\n",
            "y2bar     0\n",
            "xybar     0\n",
            "x2ybar    0\n",
            "xy2bar    0\n",
            "xedge     0\n",
            "xedgey    0\n",
            "yedge     0\n",
            "yedgex    0\n",
            "dtype: int64\n",
            "\n",
            "Shape of scaled features (X): (20000, 16)\n",
            "Shape of one-hot encoded target (y): (20000, 26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y_one_hot, test_size=0.2, random_state=42, stratify=y_one_hot\n",
        ")\n",
        "\n",
        "print(\"Training data shape:\", X_train.shape)\n",
        "print(\"Test data shape:\", X_test.shape)\n",
        "\n",
        "# Define the basic ANN model\n",
        "basic_model = Sequential([\n",
        "    # Input layer and one hidden layer with 128 neurons and ReLU activation\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    # Output layer with 26 neurons (for 26 alphabets) and softmax activation\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "basic_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Print the model summary\n",
        "print(\"Basic ANN Model Summary:\")\n",
        "basic_model.summary()\n",
        "\n",
        "# Train the model\n",
        "print(\"\\nTraining the basic model...\")\n",
        "history_basic = basic_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "print(\"\\nEvaluating the basic model...\")\n",
        "loss, accuracy = basic_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy (Basic Model): {accuracy:.4f}\")\n",
        "\n",
        "# Make predictions and generate a classification report\n",
        "y_pred_basic_prob = basic_model.predict(X_test)\n",
        "y_pred_basic = np.argmax(y_pred_basic_prob, axis=1)\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report (Basic Model):\")\n",
        "print(classification_report(y_test_labels, y_pred_basic, target_names=encoder.classes_, zero_division=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L0cT6G9Zgv2Z",
        "outputId": "85eff730-59c6-476e-94b6-53388255df1a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (16000, 16)\n",
            "Test data shape: (4000, 16)\n",
            "Basic ANN Model Summary:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m2,176\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m)             │         \u001b[38;5;34m3,354\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,176</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,354</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,530\u001b[0m (21.60 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,530</span> (21.60 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,530\u001b[0m (21.60 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,530</span> (21.60 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training the basic model...\n",
            "Epoch 1/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.1498 - loss: 3.1469 - val_accuracy: 0.4781 - val_loss: 2.5801\n",
            "Epoch 2/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4795 - loss: 2.4210 - val_accuracy: 0.5481 - val_loss: 1.9616\n",
            "Epoch 3/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5519 - loss: 1.9067 - val_accuracy: 0.5813 - val_loss: 1.6384\n",
            "Epoch 4/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6145 - loss: 1.6045 - val_accuracy: 0.6400 - val_loss: 1.4513\n",
            "Epoch 5/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6434 - loss: 1.4365 - val_accuracy: 0.6494 - val_loss: 1.3315\n",
            "Epoch 6/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6622 - loss: 1.3228 - val_accuracy: 0.6656 - val_loss: 1.2529\n",
            "Epoch 7/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6808 - loss: 1.2490 - val_accuracy: 0.6894 - val_loss: 1.1864\n",
            "Epoch 8/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6952 - loss: 1.1857 - val_accuracy: 0.6900 - val_loss: 1.1408\n",
            "Epoch 9/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6997 - loss: 1.1505 - val_accuracy: 0.7050 - val_loss: 1.0985\n",
            "Epoch 10/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7181 - loss: 1.1029 - val_accuracy: 0.7244 - val_loss: 1.0666\n",
            "Epoch 11/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7243 - loss: 1.0843 - val_accuracy: 0.7237 - val_loss: 1.0392\n",
            "Epoch 12/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7341 - loss: 1.0350 - val_accuracy: 0.7262 - val_loss: 1.0146\n",
            "Epoch 13/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7324 - loss: 1.0180 - val_accuracy: 0.7306 - val_loss: 0.9952\n",
            "Epoch 14/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7431 - loss: 0.9920 - val_accuracy: 0.7369 - val_loss: 0.9688\n",
            "Epoch 15/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7489 - loss: 0.9753 - val_accuracy: 0.7500 - val_loss: 0.9444\n",
            "Epoch 16/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7533 - loss: 0.9469 - val_accuracy: 0.7425 - val_loss: 0.9348\n",
            "Epoch 17/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7611 - loss: 0.9268 - val_accuracy: 0.7475 - val_loss: 0.9273\n",
            "Epoch 18/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7596 - loss: 0.9355 - val_accuracy: 0.7569 - val_loss: 0.9010\n",
            "Epoch 19/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7711 - loss: 0.9027 - val_accuracy: 0.7619 - val_loss: 0.8860\n",
            "Epoch 20/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7727 - loss: 0.9035 - val_accuracy: 0.7606 - val_loss: 0.8847\n",
            "Epoch 21/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7775 - loss: 0.8677 - val_accuracy: 0.7650 - val_loss: 0.8723\n",
            "Epoch 22/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7714 - loss: 0.8776 - val_accuracy: 0.7694 - val_loss: 0.8556\n",
            "Epoch 23/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7746 - loss: 0.8498 - val_accuracy: 0.7681 - val_loss: 0.8473\n",
            "Epoch 24/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7772 - loss: 0.8511 - val_accuracy: 0.7719 - val_loss: 0.8432\n",
            "Epoch 25/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7808 - loss: 0.8456 - val_accuracy: 0.7706 - val_loss: 0.8285\n",
            "Epoch 26/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7757 - loss: 0.8482 - val_accuracy: 0.7725 - val_loss: 0.8225\n",
            "Epoch 27/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7868 - loss: 0.8123 - val_accuracy: 0.7788 - val_loss: 0.8035\n",
            "Epoch 28/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7905 - loss: 0.7920 - val_accuracy: 0.7825 - val_loss: 0.8035\n",
            "Epoch 29/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7895 - loss: 0.8041 - val_accuracy: 0.7812 - val_loss: 0.7969\n",
            "Epoch 30/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7953 - loss: 0.7910 - val_accuracy: 0.7825 - val_loss: 0.7906\n",
            "Epoch 31/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7925 - loss: 0.7911 - val_accuracy: 0.7825 - val_loss: 0.7863\n",
            "Epoch 32/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7948 - loss: 0.7690 - val_accuracy: 0.7881 - val_loss: 0.7735\n",
            "Epoch 33/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7945 - loss: 0.7704 - val_accuracy: 0.7856 - val_loss: 0.7761\n",
            "Epoch 34/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8026 - loss: 0.7504 - val_accuracy: 0.7931 - val_loss: 0.7642\n",
            "Epoch 35/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8059 - loss: 0.7515 - val_accuracy: 0.7944 - val_loss: 0.7617\n",
            "Epoch 36/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8022 - loss: 0.7539 - val_accuracy: 0.7956 - val_loss: 0.7520\n",
            "Epoch 37/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8026 - loss: 0.7436 - val_accuracy: 0.7944 - val_loss: 0.7457\n",
            "Epoch 38/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8066 - loss: 0.7305 - val_accuracy: 0.8069 - val_loss: 0.7376\n",
            "Epoch 39/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8041 - loss: 0.7313 - val_accuracy: 0.8000 - val_loss: 0.7305\n",
            "Epoch 40/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8059 - loss: 0.7180 - val_accuracy: 0.7962 - val_loss: 0.7291\n",
            "Epoch 41/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8080 - loss: 0.7181 - val_accuracy: 0.7944 - val_loss: 0.7213\n",
            "Epoch 42/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8023 - loss: 0.7157 - val_accuracy: 0.7975 - val_loss: 0.7215\n",
            "Epoch 43/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8084 - loss: 0.7052 - val_accuracy: 0.7981 - val_loss: 0.7110\n",
            "Epoch 44/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8133 - loss: 0.6955 - val_accuracy: 0.8031 - val_loss: 0.7056\n",
            "Epoch 45/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8170 - loss: 0.6805 - val_accuracy: 0.7944 - val_loss: 0.7013\n",
            "Epoch 46/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8192 - loss: 0.6698 - val_accuracy: 0.7981 - val_loss: 0.7046\n",
            "Epoch 47/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8184 - loss: 0.6652 - val_accuracy: 0.8037 - val_loss: 0.6932\n",
            "Epoch 48/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8172 - loss: 0.6631 - val_accuracy: 0.8087 - val_loss: 0.6791\n",
            "Epoch 49/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8227 - loss: 0.6552 - val_accuracy: 0.8031 - val_loss: 0.6798\n",
            "Epoch 50/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8132 - loss: 0.6707 - val_accuracy: 0.8100 - val_loss: 0.6768\n",
            "\n",
            "Evaluating the basic model...\n",
            "Test Accuracy (Basic Model): 0.8160\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
            "\n",
            "Classification Report (Basic Model):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.90      0.90      0.90       158\n",
            "           B       0.67      0.85      0.75       153\n",
            "           C       0.82      0.84      0.83       147\n",
            "           D       0.83      0.81      0.82       161\n",
            "           E       0.74      0.82      0.78       154\n",
            "           F       0.80      0.73      0.76       155\n",
            "           G       0.76      0.65      0.70       155\n",
            "           H       0.80      0.60      0.68       147\n",
            "           I       0.87      0.82      0.85       151\n",
            "           J       0.84      0.85      0.85       149\n",
            "           K       0.90      0.71      0.79       148\n",
            "           L       0.76      0.86      0.80       152\n",
            "           M       0.90      0.97      0.93       158\n",
            "           N       0.91      0.92      0.91       157\n",
            "           O       0.80      0.74      0.77       151\n",
            "           P       0.81      0.92      0.86       161\n",
            "           Q       0.75      0.79      0.77       157\n",
            "           R       0.77      0.79      0.78       151\n",
            "           S       0.74      0.58      0.65       149\n",
            "           T       0.74      0.87      0.80       159\n",
            "           U       0.87      0.93      0.90       163\n",
            "           V       0.88      0.88      0.88       153\n",
            "           W       0.89      0.89      0.89       150\n",
            "           X       0.87      0.79      0.83       157\n",
            "           Y       0.85      0.85      0.85       157\n",
            "           Z       0.78      0.85      0.81       147\n",
            "\n",
            "    accuracy                           0.82      4000\n",
            "   macro avg       0.82      0.81      0.81      4000\n",
            "weighted avg       0.82      0.82      0.81      4000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q keras-tuner\n",
        "\n",
        "import keras_tuner as kt\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Tune the number of units in the first dense layer\n",
        "    hp_units = hp.Int('units_1', min_value=32, max_value=512, step=32)\n",
        "    model.add(Dense(units=hp_units, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "\n",
        "    # Tune the number of hidden layers and their units\n",
        "    for i in range(hp.Int('num_layers', 1, 3)):\n",
        "        model.add(Dense(\n",
        "            units=hp.Int(f'units_{i+2}', min_value=32, max_value=256, step=32),\n",
        "            activation=hp.Choice('activation', ['relu', 'tanh'])\n",
        "        ))\n",
        "\n",
        "    # Add a dropout layer to prevent overfitting\n",
        "    model.add(Dropout(rate=hp.Float('dropout', 0.1, 0.5, step=0.1)))\n",
        "\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Tune the learning rate for the optimizer\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Instantiate the tuner\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,  # Number of hyperparameter combinations to try\n",
        "    executions_per_trial=2, # Number of models to train for each combination\n",
        "    directory='tuner_dir',\n",
        "    project_name='alphabet_classification'\n",
        ")\n",
        "\n",
        "print(\"KerasTuner setup complete.\")\n",
        "tuner.search_space_summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92f_5NRohEA3",
        "outputId": "30e6ff8c-dc32-4ef7-940c-743c29ead0dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hKerasTuner setup complete.\n",
            "Search space summary\n",
            "Default search space size: 6\n",
            "units_1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
            "num_layers (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 3, 'step': 1, 'sampling': 'linear'}\n",
            "units_2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
            "activation (Choice)\n",
            "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh'], 'ordered': False}\n",
            "dropout (Float)\n",
            "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.1, 'sampling': 'linear'}\n",
            "learning_rate (Choice)\n",
            "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the hyperparameter search\n",
        "print(\"\\nStarting hyperparameter search...\")\n",
        "tuner.search(X_train, y_train, epochs=20, validation_split=0.2)\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "Best Hyperparameters Found:\n",
        "- Units in first layer: {best_hps.get('units_1')}\n",
        "- Number of hidden layers: {best_hps.get('num_layers')}\n",
        "- Activation function: {best_hps.get('activation')}\n",
        "- Dropout rate: {best_hps.get('dropout')}\n",
        "- Learning rate: {best_hps.get('learning_rate')}\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "539PRgwnhXmg",
        "outputId": "11f94306-1c6b-4f42-ccd7-81c02bab43cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 01m 03s]\n",
            "val_accuracy: 0.8876562416553497\n",
            "\n",
            "Best val_accuracy So Far: 0.8876562416553497\n",
            "Total elapsed time: 00h 12m 34s\n",
            "\n",
            "Best Hyperparameters Found:\n",
            "- Units in first layer: 480\n",
            "- Number of hidden layers: 2\n",
            "- Activation function: tanh\n",
            "- Dropout rate: 0.30000000000000004\n",
            "- Learning rate: 0.01\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with the best hyperparameters and train it\n",
        "tuned_model = tuner.get_best_models(num_models=1)[0]\n",
        "history_tuned = tuned_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the tuned model\n",
        "print(\"\\nEvaluating the tuned model...\")\n",
        "loss_tuned, accuracy_tuned = tuned_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy (Tuned Model): {accuracy_tuned:.4f}\")\n",
        "\n",
        "# Make predictions and generate a classification report\n",
        "y_pred_tuned_prob = tuned_model.predict(X_test)\n",
        "y_pred_tuned = np.argmax(y_pred_tuned_prob, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report (Tuned Model):\")\n",
        "print(classification_report(y_test_labels, y_pred_tuned, target_names=encoder.classes_, zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE-LFPKyks1D",
        "outputId": "6a830a67-fbbf-4155-cfb7-169a6eff3cb4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8720 - loss: 0.4083 - val_accuracy: 0.8913 - val_loss: 0.3571\n",
            "Epoch 2/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8796 - loss: 0.3948 - val_accuracy: 0.9125 - val_loss: 0.3077\n",
            "Epoch 3/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8880 - loss: 0.3704 - val_accuracy: 0.8994 - val_loss: 0.3241\n",
            "Epoch 4/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8907 - loss: 0.3516 - val_accuracy: 0.8994 - val_loss: 0.3077\n",
            "Epoch 5/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8909 - loss: 0.3458 - val_accuracy: 0.9137 - val_loss: 0.2895\n",
            "Epoch 6/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8887 - loss: 0.3804 - val_accuracy: 0.9075 - val_loss: 0.2880\n",
            "Epoch 7/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8918 - loss: 0.3460 - val_accuracy: 0.9050 - val_loss: 0.3021\n",
            "Epoch 8/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8981 - loss: 0.3315 - val_accuracy: 0.9150 - val_loss: 0.2792\n",
            "Epoch 9/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8967 - loss: 0.3360 - val_accuracy: 0.9112 - val_loss: 0.2873\n",
            "Epoch 10/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8966 - loss: 0.3410 - val_accuracy: 0.9119 - val_loss: 0.2775\n",
            "Epoch 11/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9010 - loss: 0.3077 - val_accuracy: 0.9137 - val_loss: 0.2697\n",
            "Epoch 12/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9033 - loss: 0.3193 - val_accuracy: 0.9125 - val_loss: 0.3039\n",
            "Epoch 13/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8984 - loss: 0.3378 - val_accuracy: 0.9212 - val_loss: 0.2471\n",
            "Epoch 14/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9064 - loss: 0.3098 - val_accuracy: 0.9038 - val_loss: 0.3085\n",
            "Epoch 15/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9035 - loss: 0.3126 - val_accuracy: 0.9219 - val_loss: 0.2471\n",
            "Epoch 16/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8980 - loss: 0.3313 - val_accuracy: 0.9131 - val_loss: 0.3009\n",
            "Epoch 17/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9051 - loss: 0.3069 - val_accuracy: 0.9294 - val_loss: 0.2453\n",
            "Epoch 18/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9101 - loss: 0.3003 - val_accuracy: 0.9106 - val_loss: 0.3011\n",
            "Epoch 19/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9077 - loss: 0.3048 - val_accuracy: 0.9144 - val_loss: 0.2711\n",
            "Epoch 20/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9052 - loss: 0.2997 - val_accuracy: 0.9144 - val_loss: 0.2951\n",
            "Epoch 21/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9099 - loss: 0.2889 - val_accuracy: 0.9169 - val_loss: 0.2808\n",
            "Epoch 22/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9027 - loss: 0.3163 - val_accuracy: 0.9150 - val_loss: 0.2826\n",
            "Epoch 23/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9073 - loss: 0.3018 - val_accuracy: 0.9400 - val_loss: 0.1960\n",
            "Epoch 24/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9174 - loss: 0.2711 - val_accuracy: 0.9075 - val_loss: 0.3303\n",
            "Epoch 25/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9034 - loss: 0.3192 - val_accuracy: 0.9244 - val_loss: 0.2480\n",
            "Epoch 26/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9129 - loss: 0.2853 - val_accuracy: 0.9187 - val_loss: 0.2673\n",
            "Epoch 27/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9224 - loss: 0.2472 - val_accuracy: 0.9119 - val_loss: 0.2744\n",
            "Epoch 28/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9160 - loss: 0.2650 - val_accuracy: 0.9044 - val_loss: 0.3254\n",
            "Epoch 29/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9142 - loss: 0.2797 - val_accuracy: 0.8919 - val_loss: 0.3557\n",
            "Epoch 30/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9190 - loss: 0.2730 - val_accuracy: 0.9237 - val_loss: 0.2552\n",
            "Epoch 31/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9177 - loss: 0.2694 - val_accuracy: 0.9106 - val_loss: 0.3163\n",
            "Epoch 32/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9126 - loss: 0.2998 - val_accuracy: 0.9312 - val_loss: 0.2339\n",
            "Epoch 33/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9163 - loss: 0.2752 - val_accuracy: 0.9250 - val_loss: 0.2482\n",
            "Epoch 34/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9128 - loss: 0.2899 - val_accuracy: 0.9194 - val_loss: 0.2791\n",
            "Epoch 35/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9145 - loss: 0.2817 - val_accuracy: 0.9125 - val_loss: 0.3019\n",
            "Epoch 36/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9190 - loss: 0.2584 - val_accuracy: 0.9244 - val_loss: 0.2535\n",
            "Epoch 37/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9166 - loss: 0.2628 - val_accuracy: 0.9337 - val_loss: 0.2161\n",
            "Epoch 38/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9084 - loss: 0.3019 - val_accuracy: 0.9381 - val_loss: 0.2141\n",
            "Epoch 39/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9219 - loss: 0.2613 - val_accuracy: 0.9137 - val_loss: 0.2917\n",
            "Epoch 40/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9142 - loss: 0.2845 - val_accuracy: 0.9237 - val_loss: 0.2616\n",
            "Epoch 41/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9262 - loss: 0.2422 - val_accuracy: 0.9219 - val_loss: 0.2572\n",
            "Epoch 42/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9141 - loss: 0.2809 - val_accuracy: 0.9194 - val_loss: 0.2846\n",
            "Epoch 43/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9151 - loss: 0.2770 - val_accuracy: 0.9094 - val_loss: 0.3167\n",
            "Epoch 44/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9116 - loss: 0.2907 - val_accuracy: 0.9075 - val_loss: 0.3389\n",
            "Epoch 45/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9209 - loss: 0.2586 - val_accuracy: 0.9112 - val_loss: 0.2770\n",
            "Epoch 46/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9196 - loss: 0.2664 - val_accuracy: 0.9281 - val_loss: 0.2325\n",
            "Epoch 47/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9183 - loss: 0.2664 - val_accuracy: 0.9225 - val_loss: 0.2649\n",
            "Epoch 48/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9232 - loss: 0.2427 - val_accuracy: 0.9319 - val_loss: 0.2265\n",
            "Epoch 49/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9173 - loss: 0.2671 - val_accuracy: 0.9169 - val_loss: 0.2946\n",
            "Epoch 50/50\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9257 - loss: 0.2502 - val_accuracy: 0.9144 - val_loss: 0.2974\n",
            "\n",
            "Evaluating the tuned model...\n",
            "Test Accuracy (Tuned Model): 0.9247\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\n",
            "Classification Report (Tuned Model):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.97      0.96      0.97       158\n",
            "           B       0.90      0.86      0.88       153\n",
            "           C       0.95      0.93      0.94       147\n",
            "           D       0.92      0.94      0.93       161\n",
            "           E       0.95      0.88      0.91       154\n",
            "           F       0.91      0.90      0.90       155\n",
            "           G       0.80      0.88      0.84       155\n",
            "           H       0.84      0.83      0.84       147\n",
            "           I       0.96      0.85      0.90       151\n",
            "           J       0.91      0.93      0.92       149\n",
            "           K       0.93      0.91      0.92       148\n",
            "           L       0.96      0.95      0.96       152\n",
            "           M       1.00      0.97      0.99       158\n",
            "           N       0.97      0.95      0.96       157\n",
            "           O       0.88      0.92      0.90       151\n",
            "           P       0.92      0.96      0.94       161\n",
            "           Q       0.81      0.96      0.88       157\n",
            "           R       0.85      0.93      0.89       151\n",
            "           S       0.91      0.91      0.91       149\n",
            "           T       0.99      0.91      0.95       159\n",
            "           U       0.96      0.99      0.98       163\n",
            "           V       0.95      0.95      0.95       153\n",
            "           W       0.94      0.97      0.95       150\n",
            "           X       0.95      0.89      0.92       157\n",
            "           Y       0.96      0.96      0.96       157\n",
            "           Z       0.99      0.95      0.97       147\n",
            "\n",
            "    accuracy                           0.92      4000\n",
            "   macro avg       0.93      0.92      0.92      4000\n",
            "weighted avg       0.93      0.92      0.93      4000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discuss the performance differences between the model with default hyperparameters and the tuned model, emphasizing the effects of hyperparameter tuning.**\n",
        "\n",
        "The transition from the basic model to the tuned model demonstrates the critical importance of hyperparameter tuning in machine learning. The default model serves as a solid starting point, but its \"one-size-fits-all\" architecture is rarely optimal. Through hyperparameter tuning, we systematically explore different configurations—adjusting the network's depth and width, the learning rate, and regularization techniques like dropout. This process allows the model to better adapt to the specific patterns in the dataset. As a result, the tuned model almost always shows superior performance, with higher accuracy, precision, and recall, because it has found an architecture that more effectively balances the trade-off between learning from the data (low bias) and generalizing to new, unseen examples (low variance)"
      ],
      "metadata": {
        "id": "PR8PXaCkmzja"
      }
    }
  ]
}